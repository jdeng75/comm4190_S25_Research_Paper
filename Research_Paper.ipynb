{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb4f5c4-7caf-4a22-8acc-cde24395c8f4",
   "metadata": {},
   "source": [
    "# Simulating Tool for Doctor-Patient Interaction: An Analysis of AI Chatbots in Medical Care Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c92784-684a-4275-b60e-d150b07f2042",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb61cdd-ebac-4cc8-ac4a-08d73ae311cd",
   "metadata": {},
   "source": [
    "As artificial intelligence technologies, especially Large Language Models (LLMs), become increasingly integrated into everyday life, their influence on how people prepare for and understand healthcare interactions is becoming more significant. LLMs such as ChatGPT, Claude, Deepseek, and others are now capable of engaging in fluid, context-aware conversations. This makes them potentially useful tools in helping patients navigate the often overwhelming and highly asymmetrical world of doctor-patient communication.\n",
    "\n",
    "Doctor visits are typically time-constrained and emotionally charged, with many patients leaving appointments with unanswered questions, misunderstood information, or anxiety about what they should do next. These issues are especially pronounced for individuals with lower health literacy, language barriers, or communication anxiety. In this context, LLMs might offer low-cost, always-available support to help patients prepare for their appointments, clarify medical language, and even practice expressing sensitive concerns.\n",
    "\n",
    "This paper focuses on the communicative role LLMs could play in supporting, not replacing, doctor-patient interaction. Drawing from relevant literature and a series of simulated patient-AI interactions, I analyze how current LLMs perform in this role, what kinds of communication they facilitate, and what limitations or risks they pose. My goal is to critically explore how AI might shape the future of healthcare communication from the patient's perspective. \n",
    "\n",
    "For my patient-AI interactions, I will be simulating a patient who recently got diagnosed with stage 4 non-Hodgkin's lymphoma and is trying to understand their diagnosis better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d65ee-a680-4c21-871d-15d383fa5fc9",
   "metadata": {},
   "source": [
    "## Literature Review: AI Applications in Doctor-Patient Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d4553-57a6-42ae-a8b1-3b3be1d10665",
   "metadata": {},
   "source": [
    "AI is increasingly being applied in healthcare, particularly in interactions between patients and providers. From AI-powered chatbots that handle symptom triage and appointment scheduling to intelligent systems that assist with patient communication and engagement, these technologies promise to streamline care and improve patient experiences. At the same time, they raise ethical concerns around data privacy, transparency, and accountability.\n",
    "\n",
    "AI is transforming how patients and providers communicate outside of direct consultations. New York's **Northwell Health** has deployed \"Health Chats,\" an AI text messaging program for patients with chronic conditions or post-surgery recovery. The chat is tailored to each patient’s condition and history (Boyle 2023). For example, a postpartum patietns might get asked daily questions about bleeding and mood, while others can check in on their other conditions. The chatbot engages the patient with multiple-choice or free-text questions and follows decision trees based on the responses.\n",
    "\n",
    "There are also patient portals and AI-assisted messaging. With the rise of electronic health record patient portals (such as MyChart) in recent years, patients often send their care team questions: “Is my test result normal?”, “I need a refill”, “Should I be worried about this symptom?” The volume of these messages skyrocketed during the pandemic and has remained high, contributing to physician workload and burnout (Boyle 2023). For example, UC San Diego Health and UNC Health have piloted a system where an AI chatbot is integrated with their MyChart portal. When a patient message comes in, the AI can parse the content and generate a suggested response for the provider.\n",
    "\n",
    "There is evidence that AI can draft high-quality responses. A study in 2023 (at UCSD) took real patient questions posted online and had ChatGPT generate answers, then compared those to answers from physicians. Blinded evaluators preferred the chatbot’s answers 79% of the time, rating them as more empathetic and thorough on average (Boyle 2023).\n",
    "\n",
    "AI’s ability to process large amounts of data can be harnessed to personalize patient communications at scale. For instance, machine learning algorithms can analyze a patient’s medical records, demographics, and even psychographic data to tailor educational content and motivational messages. An AI system might learn which patients prefer direct succinct communication versus those who engage better with a friendly, chatty tone, and adjust accordingly ('Revolutionizing Patient Engagement').\n",
    "\n",
    "However, there are some ethical and regulatory challenges.\n",
    "AI systems in healthcare often require access to sensitive patient information to function effectively. A chatbot taking a medical history or a voice assistant listening in a hospital room will handle personal health data. This raises the stakes for data protection. It is essential that robust measures are in place to safeguard any data AI collects or generates. According to the CDC, with AI’s ability to process vast amounts of personal data, “safeguarding patient privacy and confidentiality becomes paramount” (Dankwa-Mullan 2024). The CDC is also worried about patient autonomy and informed consent.  The CDC suggests making consent forms clear about AI: patients should be given “concise language for how their data will be used in AI systems to inform their care” (Dankwa-Mullan 2024).\n",
    "\n",
    "There's also bias and health equity. Algorithmic bias might mean the AI works better for some groups (e.g., middle-aged men) and worse for others (e.g., elderly women or racial minorities) if those groups were underrepresented or misrepresented in the training data (Dankwa-Mullan 2024).\n",
    "\n",
    "\n",
    "A stark reminder of the ethical stakes in AI-assisted patient interactions came in 2023 with the case of “Tessa,” an eating-disorder chatbot. The National Eating Disorders Association (NEDA) had introduced Tessa as a support chatbot after scaling back their human-staffed helpline. However, Tessa began giving harmful advice: when users said they were struggling with weight and body image, the bot suggested tips on how to cut calories and lose weight – terribly misguided advice for someone with or at risk of an eating disorder (Hoover 2023).\n",
    "\n",
    "AI holds great promise for making doctor-patient interactions more efficient, responsive, and personalized. A well-designed health chatbot can provide patients with quick triage advice or health information in the middle of the night, improving access to care and potentially reducing unnecessary clinic visits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ca07d-f2a6-432c-b755-8b5c785a2c01",
   "metadata": {},
   "source": [
    "## Case Study: Diagnosis Conversations with AI Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa8efa-4779-4a1e-a62d-68c4a1ebf326",
   "metadata": {},
   "source": [
    "To understand the basis of this project and first-hand insight into the experience of using AI chatbots for diagnosis discussions for patients, I wanted to engage with various LLMs to see the difference in their response: **Claude-3.7, ChatGPT-4.1, Deepseek.**\n",
    "\n",
    "To minimize any random variation in responses, I stuck to three questions to prompt each LLM.\n",
    "\n",
    "1. \"I was diagnosed with stage 4 non-Hodgkin's lymphoma by my doctor. I'm scared. What is this, how serious is it, what should I do, what should I ask my doctor?\n",
    "2. What is R-chop, lymphadenopathy, and immunophenotyping?\n",
    "3. Is all of this safe?\n",
    "\n",
    "\n",
    "This ensures that the model understands my specific diagnosis. With the second prompt, I use these questions to understand if each model can efficiently explain these medical jargons. And for the third prompt, I want to use this question to see how \"empathetic\" the model can be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f271e52-35ac-4e4f-959a-d9497759b944",
   "metadata": {},
   "source": [
    "### Coversations with Claude-3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ff2b6-d4ed-41c5-b367-24bb5a273356",
   "metadata": {},
   "source": [
    "Initially, I used anthropic/claude-3.7. My first interaction was framed around a common patients experience: anxiety and uncertainty before another doctor's visit. I prompted Claude with: \n",
    "</br>\n",
    "\n",
    "> I was diagnosed with stage 4 non-Hodgkin's lymphoma by my doctor. I'm scared. What is this, how serious is it, what should I do, what should I ask my doctor?\"\n",
    "\n",
    "The first thing I noticed is the first sentence. It was bolded and in larger font than the rest of the other fonts, saying, **\"I'm sorry to hear about your diagnosis.\"** This model was highly empathetic, saying how it wanted to first acknowledge the diagnosis.\n",
    "\n",
    "Aside from the empathetic interaction, Claude's response was highly structured and supportive, offering a set of questions organized around system tracking, life, and potential. It suggested what to do next:\n",
    "\n",
    "- Connect with doctor\n",
    "- take notes\n",
    "- consider a second opinion\n",
    "\n",
    "Claude also offered meta-level communication adive: it encouraged me to write down my questions ahead of time and share a written list with the provider. This attention to process, not jjust content, stood out as a reflection of Claude's human-centered design.\n",
    "\n",
    "I also like how it structured **\"questions to ask your doctor\"**:\n",
    "\n",
    "- What is my treatment plan\n",
    "- How will treatment affect my daily life?\n",
    "\n",
    "There are more useful responses that Claude-3.7 created. All of which a normal patient would ask their physicians.\n",
    "\n",
    "Claude maintained a cautious yet affirming tone. It offered  sampled language use and emphasized the importance of self-advocacy in healthcare. It maintained boundaries while still offering support.\n",
    "\n",
    "As for helping me understand the key terms in this diagnosis, it was also well structured and short. It would explain the term, and explain how it was relevant to my diagnosis.\n",
    "\n",
    "When asking if these ere all safe, it kept the responses short and kept emphasizing that I should talk to my physician.\n",
    "\n",
    "Overall, Claude-3.7's communication support was empathetic, articulate, and boundaries-aware. It demonstrated how a well-trained LLM can **simulate relational scaffolding without overstepping into dangerous territory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebb687-3f51-404f-9d68-0bf0688992e0",
   "metadata": {},
   "source": [
    "<img src=\"Claude-3.7.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd96657-9ddb-4a21-8503-a6962a012609",
   "metadata": {},
   "source": [
    "### Conversations with ChatGPT-4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77cf06-dbdf-4b7d-83fc-d5239d4f541c",
   "metadata": {},
   "source": [
    "GPT-4.1 feels similarly structured to Claude 3.7, but its responses are even more concise. Unlike Claude, GPT-4.1 didn't start with a bolded heading with encouraging words, but it did open with a compassionate statement: **\"I'm very sorry to hear about your diagnosis.\"**\n",
    "\n",
    "The **What should you do** section mirrored Claude's suggestions. \n",
    "\n",
    "- Stay informed\n",
    "- Talk to a doctor\n",
    "- Consider a second option\n",
    "- Prioritize self-care\n",
    "\n",
    "However, GPT-4.1 stood out in how it organized follow-up questions for a doctor. The questions were grouped by themes: treatment, diagnosis, support, and more. I found this format clearer and more useful than Claude's.\n",
    "\n",
    "The **Final Thoughts** section from GPT-4.1 felt more intense. It emphasized the seriousness of the diagnosis several times, which made me feel a bit uneasy. I don't think I would like to hear about how serious it was... especially if I knew I had stage 4 cancer. Still, it offered reassurance by reminding me that I'm not alone and that help is available.\n",
    "\n",
    "The explanation of medical jargon was noticeably shorter and more simplified than Claude's. I think this was intentional so it wouldn't overwhelm a patient or risk the delivery of inaccurate information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e09231-7ec5-4f05-8b11-f4b0ddef968b",
   "metadata": {},
   "source": [
    "<img src=\"GPT-4.1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0df6e6-7ac5-4a01-a3f7-d8e0a049db1c",
   "metadata": {},
   "source": [
    "### Conversations with DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23db84-c33c-409d-b5ae-fdb0f2663fdc",
   "metadata": {},
   "source": [
    "Deepseek opened in a similar way to GPT-4.1, expressing condolences and acknowledging how scary this time must be. Like the others, it emphasized the seriousness of the diagnosis—but perhaps even more directly.\n",
    "\n",
    "One standout difference was Deepseek’s dedicated section titled **“How Serious Is Stage 4 NHL?”** This was more explicit and potentially more alarming than what the other models included. While honest, it might be overwhelming for some patients.\n",
    "\n",
    "In the **“What Should You Do Now?”** section, Deepseek offered slightly different guidance. It focused more on gaining clarity, discussing treatment options, asking about prognosis, and caring for oneself. Notably, it did not mention seeking a second opinion, which both GPT-4.1 and Claude did. Instead, it leaned toward more concrete next steps, like considering CAR T-cell therapy—showing a more medically action-oriented approach.\n",
    "\n",
    "The section explaining medical terms included a helpful addition: ((“Why it matters for you.”(( This personalized context helped make the information feel more relevant and digestible, setting it apart from GPT-4.1 and Claude.\n",
    "\n",
    "While the doctor question prompts were similar across all models, Deepseek stood out for including a Hope and Support section. This kind of explicit emotional reassurance was less visible in GPT-4.1 and Claude, and I appreciated its inclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b5139-59ef-444c-92e6-3642756b92ac",
   "metadata": {},
   "source": [
    "<img src=\"Deepseek.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c4ccb-cdc0-4424-b918-2c4de5a0b3b4",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00841456-f26c-430f-a3fc-7b84f63928f9",
   "metadata": {},
   "source": [
    "The conversations with Claude-3.7, ChatGPT-4.1, and Deepseek shows the benefits and negatives of using AI chatbox in a diagnosis setting for doctor-patient interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96fd158-d7d1-4807-980b-a84ebeef20aa",
   "metadata": {},
   "source": [
    "The benefits of using AI chatboys in doctor-patient interactions are extensive.\n",
    "\n",
    "1. **Emotional Support and Reassurance**: They all begin interactions with empathetic, supportive language, which can be comforting for patients experiencing fear or uncertainty.\n",
    "2. **Structured Guidance and Information**: The models provide organized action steps, which can help patients feel more in control during a stressful time.\n",
    "3. **More Clear Medical Jargon Explanations**: Most chatbots simplify complex terminology and explain why certain terms matter, which supports patient understanding without overwhelming them.\n",
    "4. **Thoughtful Question Prompts for Doctor**: By offering structured questions to ask physicians, AI chatboxes can empower patietns to have more productive medical consultations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8185f-e63d-4021-9063-94f626797748",
   "metadata": {},
   "source": [
    "However, there are some negative sides too...\n",
    "\n",
    "1. **Emphasis on Seriousness May Increase Anxiety**:While it’s important to be honest, the repeated emphasis on the seriousness of a diagnosis (especially as seen in Deepseek) can heighten fear or emotional distress.\n",
    "\n",
    "2. **Lack of Personalization or Nuance**:\n",
    "AI cannot fully tailor its response to a patient’s unique emotional state or medical history, which could result in generic or even misleading advice.\n",
    "\n",
    "3. **Absence of Certain Medical Recommendations**:\n",
    "Some models omit important guidance,like the suggestion to seek a second opinion,which could impact patient decision-making.\n",
    "\n",
    "4. **Risk of Overstepping Clinical Boundaries**:\n",
    "When AI recommends specific treatments (e.g., CAR T-cell therapy), it may unintentionally give premature or inappropriate advice without full context, potentially leading to misinformation.\n",
    "\n",
    "5. **Potential for Emotional Missteps**:\n",
    "Even well-meaning reassurance (like “you’re not alone”) might feel impersonal or inadequate to some users, especially when not backed by real human presence.\n",
    "\n",
    "6. **No Follow-Up or Accountability**:\n",
    "AI tools can’t follow up, track progress, or clarify misunderstandings, unlike a human doctor who can adapt in real-time based on patient reactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1bcbc1-6eb3-42d5-8cae-3188c581af27",
   "metadata": {},
   "source": [
    "Let us remind ourselves that the purpose of AI tools in diagnosis settings with doctor-patient interactions is not to replace a doctor's job. It is only to enhance and act as a useful, effective tool for patients who want to gain more control of their health. When we place patients at the center and allow them to ask unlimited questions, it helps improve healthcare for all and use **AI for good.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
